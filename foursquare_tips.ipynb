{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Teste NLP Pagseguro - Eduarth Heinen",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOPykATaG1Rq3n0pmvcRDbt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduartheinen/foursquare-tips/blob/master/foursquare_tips.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44FLg7CDfD-g"
      },
      "source": [
        "!pip install pip setuptools spacy wheel xgboost\n",
        "# !python -m spacy download pt_core_news_sm # comment this line after first run"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r6-1fBl--hf"
      },
      "source": [
        "import re\n",
        "import string\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_selection import chi2, SelectKBest\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIylpfff8g_N"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_GEj9iqEw2i"
      },
      "source": [
        "class FoursquareTipsDataset:\n",
        "    def __init__(self, df, ngram_range=None):\n",
        "        # extracting lemmas and POS tags with spacy even though we are not using them yet\n",
        "        self.sentences, self.lemmas, self.pos = self.preprocess(df.texto)\n",
        "        self.labels = df.rotulo\n",
        "\n",
        "        # bag of words\n",
        "        self.count_vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
        "        self.bow = self.count_vectorizer.fit_transform(self.sentences)\n",
        "\n",
        "        # tfidf\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(ngram_range=ngram_range)\n",
        "        self.tfidf = self.tfidf_vectorizer.fit_transform(self.sentences)\n",
        "\n",
        "        # for easy indexing \n",
        "        self.sentences = pd.DataFrame(self.sentences)\n",
        "        self.lemmas = pd.DataFrame(self.lemmas)\n",
        "        self.pos = pd.DataFrame(self.pos)\n",
        "        \n",
        "        # wouldnt be possible with a full dataset\n",
        "        # TODO: implement smart indexing with getitem, x, and y methods\n",
        "        # self.bow = pd.DataFrame(self.bow.toarray())\n",
        "        # self.tfidf = pd.DataFrame(self.tfidf.toarray())\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess(reviews):\n",
        "        sentences = []\n",
        "        lemmas = []\n",
        "        pos = []\n",
        "\n",
        "        for sentence in tqdm(reviews):\n",
        "            sentence = re.sub(r'http\\S+', '', sentence)  # removes urls before punctuation\n",
        "            punctuation_to_space = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
        "            sentence = sentence.translate(punctuation_to_space)  # change punctuations to spaces\n",
        "            sentence = str.lower(sentence)\n",
        "            sentence = re.sub(' +', ' ', sentence)  # removes double spaces\n",
        "            sentence = re.sub('\\d+', '', sentence)  # removes numbers\n",
        "            \n",
        "            # spacy processing -- nlp(sentence) -- adds properties to words, \n",
        "            # like \"lemma_\", \"pos_\" and \"is_stop\" for stop_words.\n",
        "            sentence = list(filter(lambda w: not w.is_stop, nlp(sentence)))\n",
        "            lemmas.append([w.lemma_ for w in sentence if not w.is_stop])\n",
        "            pos.append([w.pos_ for w in sentence if not w.is_stop])\n",
        "            \n",
        "            # sklearn count/tfidf vectorizers require raw text\n",
        "            sentences.append(' '.join([w.text for w in sentence]))\n",
        "\n",
        "        return sentences, lemmas, pos\n",
        "\n",
        "    # returns train_x, train_y, test_x and test_y folds, according to cross validation indexes\n",
        "    def get_folds(self, train_index, test_index, feat=None):\n",
        "      if type(train_index) == int:\n",
        "        if feat == 'tfidf':\n",
        "          return self.tfidf.iloc[:train_index], self.labels.iloc[:train_index], self.tfidf.iloc[test_index:], self.labels.iloc[test_index:]\n",
        "        return self.bow.iloc[:train_index], self.labels.iloc[:train_index], self.bow.iloc[test_index:], self.labels.iloc[test_index:]\n",
        "      \n",
        "      else:\n",
        "        if feat == 'tfidf':\n",
        "          return self.tfidf.iloc[train_index], self.labels.iloc[train_index], self.tfidf.iloc[test_index], self.labels.iloc[test_index]\n",
        "        return self.bow.iloc[train_index], self.labels.iloc[train_index], self.bow.iloc[test_index], self.labels.iloc[test_index]\n",
        "\n",
        "\n",
        "    # TODO: implement smart indexing at __getitem__\n",
        "    def __getitem__(self, i):\n",
        "      return self.bow.iloc[i], self.tfidf.iloc[i], self.labels.iloc[i]\n",
        "    \n",
        "    def x_bow(self, i):\n",
        "      return self.bow.iloc[i]\n",
        "    \n",
        "    def x_tfidf(self, i):\n",
        "      return self.tfidf.iloc[i]\n",
        "    \n",
        "    def y(self, i):\n",
        "      return self.labels.iloc[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL5KFp4V_Ldz"
      },
      "source": [
        "### Load and Process Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtphWQXGFyy-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34a45bb6-72e6-407e-e308-32300fe42d45"
      },
      "source": [
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "path = 'https://raw.githubusercontent.com/eduartheinen/foursquare-tips/master/data/'\n",
        "df = pd.read_csv(path + 'tips_scenario1_train.csv').dropna(how='any')\n",
        "data = FoursquareTipsDataset(df, ngram_range=(1, 2))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1708/1708 [00:14<00:00, 115.87it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4DdGrku_O3H"
      },
      "source": [
        "### New Possibilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbTiWOv1_KQz"
      },
      "source": [
        "# Repositório de Word-Embeddings em Português do NILC-ICMC-USP http://www.nilc.icmc.usp.br/embeddings\n",
        "# embeddings_path = \"http://143.107.183.175:22980/download.php?file=embeddings/glove/glove_s100.zip\"\n",
        "\n",
        "# Bert treinado em Português\n",
        "# https://github.com/neuralmind-ai/portuguese-bert"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UO5uc8i8mRE"
      },
      "source": [
        "#Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eREiy-EzJjjV"
      },
      "source": [
        "feature_names = data.count_vectorizer.get_feature_names()\n",
        "support = 1\n",
        "best = None\n",
        "\n",
        "for k in range(100, 4000, 100):\n",
        "  ch2 = SelectKBest(chi2, k=k)\n",
        "  ch2.fit(data.bow, data.labels)\n",
        "  new_support = sum([ch2.scores_[i] for i in ch2.get_support(indices=True)])\n",
        "  print(f'support with {k} best features: {new_support} -- growth rate: {new_support/support}')\n",
        "  if new_support > support:\n",
        "    support = new_support\n",
        "    best = ch2\n",
        "\n",
        "# selected_features = {feature_names[i]: ch2.scores_[i] for i in ch2.get_support(indices=True)}\n",
        "# sorted(selected_features.items(), key=lambda x: x[1], reverse=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxqIzsUfstaP"
      },
      "source": [
        "for c in range(100, 2000, 100):\n",
        "  svd = TruncatedSVD(n_components=c, n_iter=10)\n",
        "  svd.fit(data.tfidf)\n",
        "  print(f'{c} components -- explained variance {svd.explained_variance_ratio_.sum()}')\n",
        "  if svd.explained_variance_ratio_.sum() > 0.98:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-4YIGNvH1Wo"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0PTHABiH0pB",
        "outputId": "327351f7-76e4-4b16-aa4c-097df1f0c084"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "kf = KFold(n_splits=10)\n",
        "kf.get_n_splits(data.bow)\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "kf = KFold(n_splits=10)\n",
        "kf.get_n_splits(data.bow)\n",
        "\n",
        "scores = []\n",
        "for train_index, test_index in kf.split(data.bow):\n",
        "  x_train, y_train, x_test, y_test = data.get_folds(train_index, test_index)\n",
        "  \n",
        "  gnb = GaussianNB()\n",
        "  gnb.fit(x_train, y_train)\n",
        "  scores.append(gnb.score(x_test, y_test))\n",
        "\n",
        "print(f'Gaussian Naive-Bayes mean score: {np.mean(scores)}')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gaussian Naive-Bayes mean score through 10 k-fold: 0.6820674234606123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "danFZrBn8JxY"
      },
      "source": [
        "#Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ij2n7wLx1gq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ea4c8d5-e7dd-44ab-aa24-d3b9c16c0ee0"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "train_index = test_index = int(len(data)*0.75) # testing with 0.25 of dataset\n",
        "x_train, y_train, x_test, y_test = data.get_folds(train_index, test_index)\n",
        "\n",
        "lr_cv = LogisticRegressionCV(cv=3, Cs=20, random_state=0, n_jobs=-1).fit(x_train, y_train)\n",
        "lr_cv.fit(x_train, y_train)#.predict(x_test)\n",
        "print(f'CV Logistic Regression mean score: {lr_cv.score(x_test, y_test)}')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "integer index.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  2.4min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  2.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CV Logistic Regression: 0.7049180327868853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qntxCNOJ8N2W"
      },
      "source": [
        "#Support Vector Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qPHhIXc8Rcu",
        "outputId": "a274e8a6-7bf8-498f-912b-5a82b705b17a"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "train_index = test_index = int(len(data)*0.75) # testing with 0.25 of dataset\n",
        "x_train, y_train, x_test, y_test = data.get_folds(train_index, test_index)\n",
        "\n",
        "scv = LinearSVC(C=1.0)\n",
        "scv.fit(x_train,y_train)\n",
        "print(f\"SVC mean score {scv.score(x_test, y_test)}\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "integer index.\n",
            "SVC mean score 0.7259953161592506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb-O7olh8R2g"
      },
      "source": [
        "#XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "nT1LcLA38VsY",
        "outputId": "e239fedf-3705-46f5-d28e-ad842008109d"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "train_index = test_index = int(len(data)*0.75) # testing with 0.25 of dataset\n",
        "x_train, y_train, x_test, y_test = data.get_folds(train_index, test_index)\n",
        "\n",
        "\n",
        "xgb = XGBClassifier()\n",
        "scores = cross_val_score(xgb, x_train, y_train)\n",
        "print(f\"XGB mean score {np.mean(scores)}\")\n",
        "\n",
        "# xgb.fit(x_train, y_train, verbose=2)\n",
        "# xgb.feature_importances_"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "integer index.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-901e49e1bb93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# scores = cross_val_score(xgb, x_train, y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"XGB mean score {scores.mean()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'mean'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZQkbrMO8WCo"
      },
      "source": [
        "#Bi-directional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "fNNi2zcxLTM5",
        "outputId": "e66a1105-86bb-4489-ce3f-2adcd5fd35b0"
      },
      "source": [
        "# tmp = pd.DataFrame(xgb.feature_importances_)\n",
        "# tmp.sort_values(ascending=False, by=0).index"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-bedd8780727b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# tmp = pd.DataFrame(xgb.feature_importances_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# tmp.sort_values(ascending=False, by=0).index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5140\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5141\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'vocab_'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guEiKA2GLTmO"
      },
      "source": [
        "#Fine Tunned BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srZtjNVULXZE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}