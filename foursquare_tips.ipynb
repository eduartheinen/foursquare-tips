{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "foursquare-tips",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMVXa5TCL1bJKyOGmclzUBc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduartheinen/foursquare-tips/blob/master/foursquare_tips.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Iela9r7n0-w"
      },
      "source": [
        "#TODO: \n",
        "1.   **GridSearch features e params**\n",
        "2.   **BERT**\n",
        "3.   **Bi-LSTM**\n",
        "3.   **Visualization of model/config results**\n",
        "4.   **ROC**\n",
        "5.   **LIME**\n",
        "6.   **Explain each model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44FLg7CDfD-g",
        "outputId": "1f7240b3-97b2-4a3c-95e3-6a852472b6c3"
      },
      "source": [
        "!pip3 install -U spacy setuptools wheel xgboost # transformers\n",
        "!python -m spacy download pt_core_news_sm # comment this line after first run"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.7/dist-packages (3.0.3)\n",
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.7/dist-packages (53.1.0)\n",
            "Requirement already up-to-date: wheel in /usr/local/lib/python3.7/dist-packages (0.36.2)\n",
            "Requirement already up-to-date: xgboost in /usr/local/lib/python3.7/dist-packages (1.3.3)\n",
            "Requirement already satisfied, skipping upgrade: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.3.2)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.7.3)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.1)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: thinc<8.1.0,>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: pathy in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.0)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy->spacy) (3.0.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy) (3.4.0)\n",
            "2021-02-25 19:04:20.000173: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "Requirement already satisfied: pt-core-news-sm==3.0.0 from https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.0.0/pt_core_news_sm-3.0.0-py3-none-any.whl#egg=pt_core_news_sm==3.0.0 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from pt-core-news-sm==3.0.0) (3.0.3)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (2.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (3.0.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (53.1.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (3.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (20.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (8.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: pathy in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (0.4.0)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (3.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy->spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (3.0.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK-h5BIoZAfP"
      },
      "source": [
        "# uncomment only if using BERT\n",
        "# !wget https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/bert-base-portuguese-cased_pytorch_checkpoint.zip\n",
        "# !wget https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/vocab.txt -P bert_checkpoint/\n",
        "# !unzip bert-base-portuguese-cased_pytorch_checkpoint.zip -d bert_checkpoint/\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r6-1fBl--hf"
      },
      "source": [
        "import re\n",
        "import string\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_selection import chi2, SelectKBest\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# pytorch\n",
        "# from torch.utils.data import Dataset, DataLoader\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIylpfff8g_N"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_GEj9iqEw2i"
      },
      "source": [
        "class FoursquareTipsDataset():\n",
        "    def __init__(self, df, ngram_range=None):\n",
        "      # extracting lemmas and POS tags with spacy even though we are not using them yet\n",
        "      self.sentences, self.terms, self.lemmas, self.pos = self.preprocess(df.texto)\n",
        "      self.labels = df.rotulo.reset_index(drop=True)\n",
        "      self.feature_type_ = 'bow'\n",
        "\n",
        "      # bag of words\n",
        "      self.count_vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
        "      self.bow = self.count_vectorizer.fit_transform(self.sentences)\n",
        "\n",
        "      # tfidf\n",
        "      self.tfidf_vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_df=0.99)\n",
        "      self.tfidf = self.tfidf_vectorizer.fit_transform(self.sentences)\n",
        "\n",
        "      # SVD/LSA\n",
        "      print('fitting bow_lsa')\n",
        "      self.svd_bow = self.fit_svd(self.bow)\n",
        "      print('fitting tfidf_lsa')\n",
        "      self.svd_tfidf = self.fit_svd(self.tfidf)\n",
        "\n",
        "      # for easy indexing\n",
        "      self.sentences = pd.DataFrame(self.sentences)\n",
        "      self.lemmas = pd.DataFrame(self.lemmas)\n",
        "      self.pos = pd.DataFrame(self.pos)\n",
        "\n",
        "    def feature_type(self, feature_type):\n",
        "      self.feature_type_ = feature_type\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess(reviews):\n",
        "        sentences = []\n",
        "        lemmas = []\n",
        "        pos = []\n",
        "        terms = []\n",
        "\n",
        "        for sentence in tqdm(reviews):\n",
        "            sentence = re.sub(r'http\\S+', '', sentence)  # removes urls before punctuation\n",
        "            punctuation_to_space = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
        "            sentence = sentence.translate(punctuation_to_space)  # change punctuations to spaces\n",
        "            sentence = str.lower(sentence)\n",
        "            sentence = re.sub('\\d+', '', sentence)  # removes numbers\n",
        "            sentence = re.sub(' +', ' ', sentence)  # removes double spaces\n",
        "\n",
        "            # spacy processing -- nlp(sentence) -- adds properties to words,\n",
        "            # like \"lemma_\", \"pos_\" and \"is_stop\" for stop_words.\n",
        "            sentence = list(filter(lambda w: not w.is_stop, nlp(sentence)))\n",
        "            lemmas.append([w.lemma_ for w in sentence if not w.is_stop])\n",
        "            pos.append([w.pos_ for w in sentence if not w.is_stop])\n",
        "            terms.append(sentence)\n",
        "\n",
        "            # sklearn count/tfidf vectorizers require raw text\n",
        "            sentences.append(' '.join([w.text for w in sentence]))\n",
        "\n",
        "        return sentences, terms, lemmas, pos\n",
        "\n",
        "    def fit_svd(self, data):\n",
        "      for c in range(1400, 2000, 100):\n",
        "        svd = TruncatedSVD(n_components=c, n_iter=10)\n",
        "        svd.fit(data)\n",
        "        if svd.explained_variance_ratio_.sum() > 0.98:\n",
        "          print(f'{c} components explained {svd.explained_variance_ratio_.sum():.4f} of feature variance.')\n",
        "          return svd.fit_transform(data)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "      if self.feature_type_ == 'svd_tfidf':\n",
        "        return self.svd_tfidf[i], self.labels.iloc[i]\n",
        "\n",
        "      if self.feature_type_ == 'svd_bow':\n",
        "        return self.svd_bow[i], self.labels.iloc[i]\n",
        "\n",
        "      if self.feature_type_ == 'tfidf':\n",
        "        return self.tfidf[i].toarray(), self.labels.iloc[i]\n",
        "\n",
        "      return self.bow[i].toarray(), self.labels.iloc[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL5KFp4V_Ldz"
      },
      "source": [
        "### Load and Process Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtphWQXGFyy-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6db3ae92-ccdf-482e-c612-2421bee9698a"
      },
      "source": [
        "nlp = spacy.load('pt_core_news_sm')\n",
        "path = 'https://raw.githubusercontent.com/eduartheinen/foursquare-tips/master/data/'\n",
        "df = pd.read_csv(path + 'tips_scenario2_train.csv').dropna(how='any')\n",
        "data = FoursquareTipsDataset(df, ngram_range=(1, 2))"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1788/1788 [00:18<00:00, 97.59it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fitting bow_lsa\n",
            "1600 components explained 0.9912 of feature variance.\n",
            "fitting tfidf_lsa\n",
            "1700 components explained 0.9875 of feature variance.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4DdGrku_O3H"
      },
      "source": [
        "### New Possibilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbTiWOv1_KQz"
      },
      "source": [
        "# Repositório de Word-Embeddings em Português do NILC-ICMC-USP http://www.nilc.icmc.usp.br/embeddings\n",
        "# embeddings_path = \"http://143.107.183.175:22980/download.php?file=embeddings/glove/glove_s100.zip\"\n",
        "\n",
        "# Bert treinado em Português\n",
        "# https://github.com/neuralmind-ai/portuguese-bert"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UO5uc8i8mRE"
      },
      "source": [
        "#Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "KutgVR_hwevl",
        "outputId": "6c6cc229-f1cd-4a33-8c11-9278e017deda"
      },
      "source": [
        "import plotly.express as px\n",
        "\n",
        "\n",
        "tmp = pd.DataFrame({'class': ['negativo', 'positivo'], \n",
        "                   'samples': [len(data.labels[data.labels==c]) for c in [-1, 1]]})\n",
        "fig = px.bar(tmp, x='class', y='samples', color='class')\n",
        "fig.show()"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"f4d366a7-c20c-4f86-a38f-953e81a0f7b0\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"f4d366a7-c20c-4f86-a38f-953e81a0f7b0\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'f4d366a7-c20c-4f86-a38f-953e81a0f7b0',\n",
              "                        [{\"alignmentgroup\": \"True\", \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"class=%{x}<br>samples=%{y}\", \"legendgroup\": \"class=negativo\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"class=negativo\", \"offsetgroup\": \"class=negativo\", \"orientation\": \"v\", \"showlegend\": true, \"textposition\": \"auto\", \"type\": \"bar\", \"x\": [\"negativo\"], \"xaxis\": \"x\", \"y\": [886], \"yaxis\": \"y\"}, {\"alignmentgroup\": \"True\", \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"class=%{x}<br>samples=%{y}\", \"legendgroup\": \"class=positivo\", \"marker\": {\"color\": \"#EF553B\"}, \"name\": \"class=positivo\", \"offsetgroup\": \"class=positivo\", \"orientation\": \"v\", \"showlegend\": true, \"textposition\": \"auto\", \"type\": \"bar\", \"x\": [\"positivo\"], \"xaxis\": \"x\", \"y\": [902], \"yaxis\": \"y\"}],\n",
              "                        {\"barmode\": \"relative\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"categoryarray\": [\"negativo\", \"positivo\"], \"categoryorder\": \"array\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"class\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"samples\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f4d366a7-c20c-4f86-a38f-953e81a0f7b0');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTM1MExXTl1f"
      },
      "source": [
        "###*Class-Balanced Loss Based on Effective Number of Samples*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fWXj7YQY_aw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bf9dc16-f17b-4fd8-d9c7-b974a75b84b4"
      },
      "source": [
        "N = data.bow.shape[0] * data.bow.shape[1]\n",
        "beta = (N - 1) / N\n",
        "# num_classes = 3\n",
        "num_classes = 2\n",
        "samples_per_class = [len(data.labels[data.labels == -1]), \\\n",
        "                    #  len(data.labels[data.labels == 0]), \\\n",
        "                     len(data.labels[data.labels == 1])]\n",
        "\n",
        "effective_num = 1.0 - np.power(beta, samples_per_class)\n",
        "weights = (1.0 - beta) / np.array(effective_num)\n",
        "weights = weights / np.sum(weights) * num_classes\n",
        "\n",
        "print(samples_per_class)\n",
        "print(weights)\n",
        "\n",
        "class_weights = {-1:weights[0], 0:1, 1:weights[1]}\n",
        "class_weights"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[414, 1161]\n",
            "[1.47428118 0.52571882]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{-1: 1.4742811750507787, 0: 1, 1: 0.5257188249492214}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvR6343wsidc",
        "outputId": "8b7d2fcc-3b6e-4420-8fe2-b94e33720774"
      },
      "source": [
        "kf = KFold(n_splits=10)\n",
        "kf.get_n_splits(data.bow)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1788, 19684)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-4YIGNvH1Wo"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0PTHABiH0pB"
      },
      "source": [
        "from sklearn.naive_bayes import ComplementNB\n",
        "features = ['bow', 'tfidf']\n",
        "scores = {f:[] for f in features}\n",
        "\n",
        "for train_index, test_index in kf.split(data.bow):\n",
        "  for f in features:\n",
        "    data.feature_type(f)\n",
        "    x_train, y_train = data[train_index]\n",
        "    x_test, y_test = data[test_index]\n",
        "    \n",
        "    cnb = ComplementNB()\n",
        "    cnb.fit(x_train, y_train)\n",
        "    scores[f].append(cnb.score(x_test, y_test))\n",
        "\n",
        "cnb_scores = scores\n",
        "nb_bow = np.mean(scores['bow'])\n",
        "nb_tfidf = np.mean(scores['tfidf'])\n",
        "print(f'Naive Bayes: bow {nb_bow:.4f}, tfidf {nb_tfidf:.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "danFZrBn8JxY"
      },
      "source": [
        "#Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "fdsoFJcxA8mE",
        "outputId": "4e69778d-6abc-4b90-c1d5-f6996b7ba33c"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "features = ['bow', 'svd_bow', 'tfidf', 'svd_tfidf']\n",
        "\n",
        "scores = {f:[] for f in features}\n",
        "# scores.update({f+'_weighted':[] for f in features})\n",
        "\n",
        "train_index = test_index = int(len(data)*0.75) # testing with 0.15 of dataset\n",
        "# for train_index, test_index in kf.split(data.bow):\n",
        "for f in features:\n",
        "  data.feature_type(f)\n",
        "  x_train, y_train = data[:train_index]\n",
        "  x_test, y_test = data[test_index:]\n",
        "\n",
        "  lr_cv = LogisticRegression(C=1e3, n_jobs=-1, max_iter=500)\n",
        "  lr_cv.fit(x_train, y_train)\n",
        "  scores[f].append(lr_cv.score(x_test, y_test))\n",
        "\n",
        "  # lr_cv = LogisticRegression(C=1, n_jobs=-1, max_iter=500, \n",
        "  #                             class_weight=class_weights)\n",
        "  # lr_cv.fit(x_train, y_train)\n",
        "  # scores[f+'_weighted'].append(lr_cv.score(x_test, y_test))\n",
        "\n",
        "lr_scores = scores\n",
        "scores"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-150-c4377b0b1ce4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mlr_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mlr_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1599\u001b[0m                       \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m                       sample_weight=sample_weight)\n\u001b[0;32m-> 1601\u001b[0;31m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qntxCNOJ8N2W"
      },
      "source": [
        "#Support Vector Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qPHhIXc8Rcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e60501ae-2a4f-4edb-838c-a4bd82c82fe1"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "features = ['bow', 'svd_bow', 'tfidf', 'svd_tfidf']\n",
        "\n",
        "scores = {f:[] for f in features}\n",
        "# scores.update({f+'_weighted':[] for f in features})\n",
        "\n",
        "# train_index = test_index = int(len(data)*0.75) # testing with 0.15 of dataset\n",
        "for train_index, test_index in kf.split(data.bow):\n",
        "  for f in features:\n",
        "    data.feature_type(f)\n",
        "    x_train, y_train = data[train_index]\n",
        "    x_test, y_test = data[test_index]\n",
        "\n",
        "    scv = LinearSVC(C=1)\n",
        "    scv.fit(x_train, y_train)\n",
        "    scores[f].append(scv.score(x_test, y_test))\n",
        "\n",
        "    # scv = LinearSVC(C=1.0, class_weight=class_weights)\n",
        "    # scv.fit(x_train, y_train)\n",
        "    # scores[f+'_weighted'].append(scv.score(x_test, y_test))\n",
        "\n",
        "svc_scores = scores\n",
        "\n",
        "for f in features:\n",
        "  tmp = np.mean(scores[f])\n",
        "  print(f'{f}:{tmp}')"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bow:0.7265049274998431\n",
            "svd_bow:0.6839997489172054\n",
            "tfidf:0.7131473228297031\n",
            "svd_tfidf:0.710906408888331\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb-O7olh8R2g"
      },
      "source": [
        "#XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT1LcLA38VsY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "outputId": "6ebfa838-b4a3-4873-8d12-a85983860cc6"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "features = ['bow', 'svd_bow', 'tfidf', 'svd_tfidf']\n",
        "\n",
        "scores = {f:[] for f in features}\n",
        "scores.update({f+'_weighted':[] for f in features})\n",
        "\n",
        "train_index = test_index = int(len(data)*0.75) # testing with 0.15 of dataset\n",
        "# for train_index, test_index in kf.split(data.bow):\n",
        "for f in features:\n",
        "  data.feature_type(f)\n",
        "  x_train, y_train = data[:train_index]\n",
        "  x_test, y_test = data[test_index:]\n",
        "\n",
        "  xgb = XGBClassifier()\n",
        "  xgb.fit(x_train, y_train)\n",
        "  scores[f].append(xgb.score(x_test, y_test))"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/xgboost/sklearn.py:888: UserWarning:\n",
            "\n",
            "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[21:57:49] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-155-eec9e04b5569>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mxgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m    913\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    233\u001b[0m                           \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                           \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                           early_stopping_rounds=early_stopping_rounds)\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1280\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[1;32m   1281\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1283\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZQkbrMO8WCo"
      },
      "source": [
        "#Bi-directional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNNi2zcxLTM5"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, tag_to_ix, input_dim, hidden_dim, batch_size, num_classes):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # GloVe, but contains only vectors that refer to words on the dataset vocabulary\n",
        "        # self.word_embeds = nn.Embedding.from_pretrained(embedding_vectors)\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim // 2, num_layers=1,\n",
        "                            bidirectional=True, batch_first=True)\n",
        "\n",
        "        # Maps the output of the LSTM into label space.\n",
        "        self.hidden2label = nn.Linear(hidden_dim, self.num_classes)\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self, batch_size=None):\n",
        "        batch_size = self.batch_size if batch_size is None else batch_size\n",
        "\n",
        "        return (torch.nn.init.xavier_uniform_(torch.zeros(2, batch_size, self.hidden_dim // 2)).to(device),\n",
        "                torch.nn.init.xavier_uniform_(torch.zeros(2, batch_size, self.hidden_dim // 2)).to(device))\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        lstm_out, _ = self.lstm(sentence, self.hidden)\n",
        "        \n",
        "        lstm_feats = self.hidden2label(unpacked_lstm_out)\n",
        "        labels = F.log_softmax(lstm_feats)\n",
        "\n",
        "        return labels"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s56bt37LqeQQ"
      },
      "source": [
        "def collate(batch):\n",
        "    inputs = torch.LongTensor([item[0] for item in batch])  # inputs = pack_padded_sequence(batch)\n",
        "    # inputs = pack_padded_sequence(inputs, [len(r[r != 0]) for r in inputs], batch_first=True, enforce_sorted=False)\n",
        "\n",
        "    target = torch.LongTensor([item[1] for item in batch])  # .view(-1)\n",
        "    # target = pack_padded_sequence(target, [len(r[r != 0]) for r in target], batch_first=True, enforce_sorted=False)\n",
        "\n",
        "    text = [item[2] for item in batch]\n",
        "    return inputs, target, text\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, collate_fn=collate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8RkVy0OqfgX"
      },
      "source": [
        "def training_epoch(model, optimizer, train_loader):  # criterion, scheduler\n",
        "    model.train()\n",
        "    losses = []\n",
        "    progress_bar = tqdm(train_loader, desc='Training', leave=False)\n",
        "    for inputs, target, text in progress_bar:\n",
        "        inputs = inputs.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        # Clean old gradients\n",
        "        # optimizer.zero_grad()\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Forwards pass\n",
        "        # loss = model.neg_log_likelihood_parallel(inputs, target)\n",
        "        loss = model.neg_log_likelihood(inputs, target)\n",
        "\n",
        "        # acc = accuracy_score(model, output)\n",
        "\n",
        "        # Perform gradient descent, backwards pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Take a step in the right direction\n",
        "        optimizer.step()\n",
        "        # scheduler.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    # plot_losses(losses, 'batch #', 'neg_log_likelihood_loss', 'Training Loss')\n",
        "    return sum(losses) / len(losses)\n",
        "\n",
        "\n",
        "def validate_epoch(model, valid_loader):  # criterion, scheduler\n",
        "    model.eval()\n",
        "    mean_F = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(valid_loader, desc='Validating', leave=False)\n",
        "        for inputs, target, text in progress_bar:\n",
        "            inputs = inputs.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            # Forwards pass\n",
        "            score, best_path = model(inputs)\n",
        "\n",
        "            # Calculating the F-Score\n",
        "            positive = [i for i, t in enumerate(target.view(-1)) if t != 0]\n",
        "            predictions = (target - best_path).view(-1)\n",
        "            true_positive = [i for i, t in enumerate(predictions[positive]) if t == 0]\n",
        "\n",
        "            if len(true_positive) > 0:\n",
        "                p = len(true_positive) / len(predictions)\n",
        "                r = len(true_positive) / len(positive)\n",
        "                mean_F.append(2 * p * r / (p + r))\n",
        "\n",
        "    # mean_F = np.mean(mean_F)\n",
        "    # return mean_F if mean_F > best_F else best_F\n",
        "    return np.mean(mean_F)\n",
        "    "
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I31Ia6PCFwh"
      },
      "source": [
        "model = BiLSTM_CRF(vocab_size=len(train_data.vocab.stoi),\n",
        "                   tag_to_ix=train_data.entity_tags.stoi, embedding_vectors=train_data.vocab.vectors,\n",
        "                   embedding_dim=embedding_dim, hidden_dim=hidden_size, batch_size=batch_size).to(device)\n",
        "\n",
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                        lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "loss_file = os.path.join(path, 'losses.csv')\n",
        "train_losses = []\n",
        "best_Fs = []\n",
        "best_F = 0\n",
        "epoch = 0\n",
        "\n",
        "# resume training\n",
        "checkpoint_file = os.path.join(path, 'ner-bilstm-crf-feb20.pth')\n",
        "\n",
        "# check if saved model exists\n",
        "resume_training = False\n",
        "if os.path.isfile(checkpoint_file) and resume_training:\n",
        "    checkpoint = torch.load(checkpoint_file)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    epoch = checkpoint['epoch']\n",
        "    train_losses = checkpoint['train_losses']\n",
        "    best_Fs = checkpoint['best_Fs']\n",
        "    best_F = np.max(best_Fs)\n",
        "\n",
        "    plot_losses(train_losses, 'training epoch #', 'neg_log_likelihood_loss', 'Training Loss')\n",
        "    plot_losses(best_Fs, 'validation epoch #', 'F score', 'F score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO3Y2r7P5oWf"
      },
      "source": [
        "# training epochs\n",
        "for epoch in range(max_epochs):\n",
        "    train_loss = training_epoch(model, optimizer, train_loader)  # train_loader\n",
        "    new_F = validate_epoch(model, valid_loader)\n",
        "\n",
        "    if new_F > best_F:\n",
        "        best_F = new_F\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "      torch.save({\n",
        "          'epoch': epoch,\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'train_losses': train_losses,\n",
        "          'best_Fs': best_Fs\n",
        "      }, checkpoint_file)\n",
        "        \n",
        "\n",
        "    # with open(loss_file, 'a', newline='\\n') as csvfile:\n",
        "    #     writer = csv.writer(csvfile)\n",
        "    #     writer.writerow([epoch + 1, train_loss, new_F])\n",
        "\n",
        "    tqdm.write(\n",
        "        f'epoch #{epoch + 1:3d}\\ttrain_loss: {train_loss:.6f}\\tcurrent_F: {new_F:.6f}\\tbest_F: {best_F:.6f} \\n',\n",
        "    )\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    best_Fs.append(new_F)\n",
        "\n",
        "    epoch += 1\n",
        "\n",
        "plot_losses(train_losses, 'training epoch #', 'neg_log_likelihood_loss', 'Training Loss')\n",
        "plot_losses(best_Fs, 'validation epoch #', 'F score', 'F score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7j5k1DlnThN"
      },
      "source": [
        "checkpoint_file = os.path.join(path, 'ner-bilstm-crf-feb18.pth')\n",
        "model.eval()\n",
        "\n",
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_losses': train_losses,\n",
        "            'best_Fs': best_Fs\n",
        "        }, checkpoint_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RvsxZsQQrUR"
      },
      "source": [
        "from IPython.display import HTML\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Create figure with secondary y-axis\n",
        "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "\n",
        "# Add traces\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=list(range(len(train_losses))), y=train_losses, name=\"nlll\"),\n",
        "    secondary_y=False,\n",
        ")\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=list(range(len(best_Fs))), y=best_Fs, name=\"f-score\"),\n",
        "    secondary_y=True,\n",
        ")\n",
        "\n",
        "# Add figure title\n",
        "fig.update_layout(\n",
        "    title_text=\"Training Loss and Validation F-score\"\n",
        ")\n",
        "\n",
        "# Set x-axis title\n",
        "fig.update_xaxes(title_text=\"train-validation epochs\")\n",
        "\n",
        "# Set y-axes titles\n",
        "fig.update_yaxes(title_text=\"<b>neg-log-likelihood-loss</b>\", secondary_y=False)\n",
        "fig.update_yaxes(title_text=\"<b>f-score</b>\", secondary_y=True)\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}